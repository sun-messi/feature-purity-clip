# %%
import numpy as np
import torch
import torch.nn.functional as F
from sklearn.feature_extraction.text import TfidfVectorizer
from concurrent.futures import ThreadPoolExecutor, as_completed
import multiprocessing as mp
from tqdm import tqdm
import re

# === å‚æ•° ===
input_txt = "cc3m_human_10w.txt"
Cs_values = [0.1, 0.3, 0.5, 0.8]  # ä¸åŒçš„äº¤æ¢æ¯”ä¾‹
top_k = 10  # åœ¨ç›¸ä¼¼åº¦æœ€é«˜çš„å‰ k ä¸ªå€™é€‰ä¸­éšæœºé€‰æ‹©
num_gpus = 8  # ä½¿ç”¨çš„GPUæ•°é‡
batch_size = 1000  # æ¯ä¸ªæ‰¹æ¬¡å¤„ç†çš„æ–‡æœ¬æ•°é‡

class MultiGPUSimilarityCalculator:
    def __init__(self, num_gpus=8):
        self.num_gpus = min(num_gpus, torch.cuda.device_count())
        self.devices = [f'cuda:{i}' for i in range(self.num_gpus)]
        print(f"ğŸš€ ä½¿ç”¨ {self.num_gpus} ä¸ªGPU: {self.devices}")
        
    def compute_similarity_batch_gpu(self, tfidf_matrix, start_idx, end_idx, device_id):
        """åœ¨æŒ‡å®šGPUä¸Šè®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µçš„ä¸€ä¸ªæ‰¹æ¬¡"""
        device = torch.device(f'cuda:{device_id}')
        
        try:
            # å°†æ•°æ®ç§»åˆ°GPU
            matrix_tensor = torch.tensor(tfidf_matrix.toarray(), dtype=torch.float32).to(device)
            batch_tensor = matrix_tensor[start_idx:end_idx]
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            matrix_norm = F.normalize(matrix_tensor, p=2, dim=1)
            batch_norm = F.normalize(batch_tensor, p=2, dim=1)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity_batch = torch.mm(batch_norm, matrix_norm.t())
            
            # ç§»å›CPU
            result = similarity_batch.cpu().numpy()
            
            # æ¸…ç†GPUå†…å­˜
            del matrix_tensor, batch_tensor, matrix_norm, batch_norm, similarity_batch
            torch.cuda.empty_cache()
            
            return start_idx, end_idx, result
            
        except Exception as e:
            print(f"âŒ GPU {device_id} è®¡ç®—æ‰¹æ¬¡ [{start_idx}:{end_idx}] æ—¶å‡ºé”™: {e}")
            return start_idx, end_idx, None

def preprocess_text(text):
    """ç®€å•çš„æ–‡æœ¬é¢„å¤„ç†"""
    text = re.sub(r'[^\w\s]', ' ', text.lower())
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def preprocess_texts_parallel(texts, num_workers=None):
    """å¹¶è¡Œé¢„å¤„ç†æ–‡æœ¬"""
    if num_workers is None:
        num_workers = min(mp.cpu_count(), 16)
    
    print(f"ğŸ”„ ä½¿ç”¨ {num_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œé¢„å¤„ç†æ–‡æœ¬...")
    
    with mp.Pool(num_workers) as pool:
        processed_texts = list(tqdm(
            pool.imap(preprocess_text, texts, chunksize=1000),
            total=len(texts),
            desc="é¢„å¤„ç†æ–‡æœ¬"
        ))
    
    return processed_texts

def compute_similarity_matrix_multigpu(texts, calculator, batch_size=1000):
    """ä½¿ç”¨å¤šGPUè®¡ç®—æ–‡æœ¬é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ"""
    print("ğŸ”„ æ­£åœ¨å¹¶è¡Œé¢„å¤„ç†æ–‡æœ¬...")
    processed_texts = preprocess_texts_parallel(texts)
    
    print("ğŸ”„ æ­£åœ¨è®¡ç®— TF-IDF å‘é‡...")
    vectorizer = TfidfVectorizer(
        max_features=5000,
        stop_words='english',
        ngram_range=(1, 2),
        min_df=2,
        max_df=0.95
    )
    
    tfidf_matrix = vectorizer.fit_transform(processed_texts)
    print(f"ğŸ“Š TF-IDF çŸ©é˜µå½¢çŠ¶: {tfidf_matrix.shape}")
    
    # è®¡ç®—æ‰¹æ¬¡
    total_texts = len(texts)
    num_batches = (total_texts + batch_size - 1) // batch_size
    
    print(f"ğŸš€ å¼€å§‹å¤šGPUç›¸ä¼¼åº¦è®¡ç®—ï¼Œå…± {num_batches} ä¸ªæ‰¹æ¬¡...")
    
    # åˆå§‹åŒ–ç›¸ä¼¼åº¦çŸ©é˜µ
    similarity_matrix = np.zeros((total_texts, total_texts), dtype=np.float32)
    
    # åˆ›å»ºæ‰¹æ¬¡ä»»åŠ¡
    batch_tasks = []
    for i in range(num_batches):
        start_idx = i * batch_size
        end_idx = min((i + 1) * batch_size, total_texts)
        device_id = i % calculator.num_gpus
        batch_tasks.append((start_idx, end_idx, device_id))
    
    # ä½¿ç”¨çº¿ç¨‹æ± ç®¡ç†GPUä»»åŠ¡
    with ThreadPoolExecutor(max_workers=calculator.num_gpus) as executor:
        # æäº¤ä»»åŠ¡
        future_to_batch = {
            executor.submit(
                calculator.compute_similarity_batch_gpu,
                tfidf_matrix, start_idx, end_idx, device_id
            ): (start_idx, end_idx, device_id)
            for start_idx, end_idx, device_id in batch_tasks
        }
        
        # æ”¶é›†ç»“æœ
        completed = 0
        for future in tqdm(as_completed(future_to_batch), total=len(future_to_batch), desc="GPUè®¡ç®—è¿›åº¦"):
            start_idx, end_idx, result = future.result()
            
            if result is not None:
                similarity_matrix[start_idx:end_idx] = result
                completed += 1
            else:
                print(f"âš ï¸ æ‰¹æ¬¡ [{start_idx}:{end_idx}] è®¡ç®—å¤±è´¥")
    
    print(f"âœ… å®Œæˆ {completed}/{num_batches} ä¸ªæ‰¹æ¬¡çš„è®¡ç®—")
    return similarity_matrix

def find_similar_candidates(similarity_matrix, idx, top_k, exclude_indices=None):
    """æ‰¾åˆ°ä¸æŒ‡å®šç´¢å¼•æœ€ç›¸ä¼¼çš„å‰ top_k ä¸ªå€™é€‰"""
    similarities = similarity_matrix[idx].copy()
    similarities[idx] = -1  # æ’é™¤è‡ªèº«
    
    if exclude_indices:
        for ex_idx in exclude_indices:
            similarities[ex_idx] = -1
    
    # æ‰¾åˆ°ç›¸ä¼¼åº¦æœ€é«˜çš„ top_k ä¸ªç´¢å¼•
    top_indices = np.argsort(similarities)[-top_k:]
    # è¿‡æ»¤æ‰ç›¸ä¼¼åº¦ä¸ºè´Ÿçš„ï¼ˆå³è¢«æ’é™¤çš„ï¼‰
    top_indices = [i for i in top_indices if similarities[i] >= 0]
    
    return top_indices

def simple_similarity_swap(lines, similarity_matrix, Cs):
    """ç®€å•çš„åŸºäºç›¸ä¼¼åº¦äº¤æ¢ï¼šäº¤æ¢åˆ°åˆšå¥½è¶…è¿‡ç›®æ ‡æ¯”ä¾‹å°±åœæ­¢"""
    total_lines = len(lines)
    target_swap_count = int(Cs * total_lines)
    
    print(f"ğŸ¯ ç›®æ ‡äº¤æ¢è¡Œæ•°: {target_swap_count} (ç›®æ ‡æ¯”ä¾‹: {Cs:.1%})")
    
    final_lines = lines.copy()
    used_indices = set()
    swap_pairs = []
    swapped_lines = 0
    
    # éšæœºæ’åˆ—æ‰€æœ‰ç´¢å¼•
    all_indices = np.arange(total_lines)
    np.random.shuffle(all_indices)
    
    for source_idx in tqdm(all_indices, desc="å¯»æ‰¾äº¤æ¢å¯¹"):
        if swapped_lines >= target_swap_count:
            print(f"âœ… å·²è¾¾åˆ°ç›®æ ‡äº¤æ¢æ•°é‡ï¼Œåœæ­¢äº¤æ¢")
            break
            
        if source_idx in used_indices:
            continue
        
        # æ‰¾åˆ°ä¸å½“å‰è¡Œæœ€ç›¸ä¼¼çš„å€™é€‰
        candidates = find_similar_candidates(
            similarity_matrix, 
            source_idx, 
            top_k, 
            exclude_indices=used_indices
        )
        
        if candidates:
            # ä»å€™é€‰ä¸­éšæœºé€‰æ‹©ä¸€ä¸ªè¿›è¡Œäº¤æ¢
            target_idx = np.random.choice(candidates)
            
            # æ‰§è¡Œäº¤æ¢
            final_lines[source_idx], final_lines[target_idx] = final_lines[target_idx], final_lines[source_idx]
            
            # è®°å½•å·²ä½¿ç”¨çš„ç´¢å¼•å’Œäº¤æ¢å¯¹
            used_indices.add(source_idx)
            used_indices.add(target_idx)
            swap_pairs.append((source_idx, target_idx))
            swapped_lines += 2  # æ¯æ¬¡äº¤æ¢å½±å“2è¡Œ
    
    actual_swap_rate = swapped_lines / total_lines
    print(f"âœ… å®Œæˆäº¤æ¢: {len(swap_pairs)}å¯¹ = {swapped_lines}è¡Œ")
    print(f"ğŸ“Š å®é™…äº¤æ¢ç‡: {actual_swap_rate:.1%} (ç›®æ ‡: {Cs:.1%})")
    
    return final_lines, swap_pairs

# === ä¸»ç¨‹åº ===
def main():
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨GPUåŠ é€Ÿ")
        return
    
    print(f"ğŸ”¥ æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPU")
    
    # è¯»å–åŸå§‹æ–‡æœ¬
    print("ğŸ“– è¯»å–åŸå§‹æ–‡æœ¬...")
    with open(input_txt, "r", encoding='utf-8') as f:
        lines = f.readlines()
    
    total_lines = len(lines)
    print(f"ğŸ“Š æ€»è¡Œæ•°: {total_lines}")
    
    # åˆå§‹åŒ–å¤šGPUè®¡ç®—å™¨
    calculator = MultiGPUSimilarityCalculator(num_gpus=num_gpus)
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆåªè®¡ç®—ä¸€æ¬¡ï¼‰
    texts = [line.strip() for line in lines]
    similarity_matrix = compute_similarity_matrix_multigpu(
        texts, calculator, batch_size=batch_size
    )
    print("âœ… ç›¸ä¼¼åº¦çŸ©é˜µè®¡ç®—å®Œæˆ")
    
    # æ‰¹é‡å¤„ç†ä¸åŒçš„Cså€¼
    for Cs in Cs_values:
        print(f"\nğŸ”€ å¤„ç† Cs = {Cs} ...")
        
        # è®¾ç½®éšæœºç§å­
        np.random.seed(42)
        
        output_txt = f"cc3m_human_10w_Cs{int(Cs * 100)}_similarity_simple.txt"
        
        # === ç®€å•äº¤æ¢ï¼šäº¤æ¢åˆ°åˆšå¥½è¶…è¿‡ç›®æ ‡æ¯”ä¾‹å°±åœæ­¢ ===
        final_lines, swap_pairs = simple_similarity_swap(lines, similarity_matrix, Cs)
        
        # === å†™å…¥è¾“å‡ºæ–‡ä»¶ ===
        print("ğŸ’¾ ä¿å­˜ç»“æœ...")
        with open(output_txt, "w", encoding='utf-8') as f:
            f.writelines(final_lines)
        
        print(f"âœ… ä¿å­˜æˆåŠŸ: {output_txt}")
        
        # === æ˜¾ç¤ºäº¤æ¢ç¤ºä¾‹ ===
        if swap_pairs:
            print(f"\nğŸ“ äº¤æ¢ç¤ºä¾‹ (Cs={Cs}):")
            for i, (idx1, idx2) in enumerate(swap_pairs[:2]):
                sim_score = similarity_matrix[idx1, idx2]
                print(f"äº¤æ¢å¯¹ {i+1} (ç›¸ä¼¼åº¦: {sim_score:.3f}):")
                print(f"  ä½ç½® {idx1}: {lines[idx2].strip()[:60]}...")
                print(f"  ä½ç½® {idx2}: {lines[idx1].strip()[:60]}...")
    
    print(f"\nğŸ‰ æ‰€æœ‰æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼")
    print("ç”Ÿæˆçš„æ–‡ä»¶:")
    for Cs in Cs_values:
        print(f"  - cc3m_human_10w_Cs{int(Cs * 100)}_similarity_simple.txt")
    
    # æ¸…ç†GPUå†…å­˜
    torch.cuda.empty_cache()

if __name__ == "__main__":
    main()

# %%


# %%


# %%
import torch
import numpy as np
from collections import OrderedDict
import os
from models import CLIP_VITB16

# åŠ è½½æ¨¡å‹
def load_model(ckpt_path):
    ckpt = torch.load(ckpt_path, map_location='cpu')
    state_dict = OrderedDict()
    for k, v in ckpt['state_dict'].items():
        state_dict[k.replace('module.', '')] = v
    model = CLIP_VITB16(rand_embed=False)
    model.load_state_dict(state_dict, strict=True)
    model.eval()
    return model

# å‘é‡å½’ä¸€åŒ–
def normalize_tensor(V):
    norm = torch.norm(V, p=2, dim=1, keepdim=True)
    return V / norm

# è®¡ç®—å¹³å‡ç»å¯¹Cosineç›¸ä¼¼åº¦
def compute_avg_abs_cos_sim(V):
    V = normalize_tensor(V)
    cosine_sim = torch.matmul(V, V.T)  # (512, 512)
    cosine_sim = cosine_sim - torch.diag(torch.diag(cosine_sim))  # å»æ‰å¯¹è§’çº¿
    avg_abs_cos = torch.mean(torch.abs(cosine_sim))
    return avg_abs_cos.item()

# ä¸»ç¨‹åº
def main():
    checkpoints = {
        "C_s=0": "finetune_result_CLIP/checkpoint.pt",
        "C_s=0.1": "finetune_result_Cs10_similarity/checkpoint.pt",
        "C_s=0.3": "finetune_result_Cs30_similarity/checkpoint.pt",
        "C_s=0.5": "finetune_result_Cs50_similarity/checkpoint.pt",
        "C_s=0.8": "finetune_result_Cs80_similarity/checkpoint.pt",
    }

    Cs_values = []
    avg_abs_cos_sims = []

    for label, ckpt_path in checkpoints.items():
        print(f"å¤„ç† {label} ...")
        model = load_model(ckpt_path)
        image_proj = model.text_projection  # (768,512)
        image_proj = image_proj.T  # (512,768)

        avg_abs_cos_sim = compute_avg_abs_cos_sim(image_proj)

        # è®°å½•
        Cs = float(label.split('=')[1])
        Cs_values.append(Cs)
        avg_abs_cos_sims.append(avg_abs_cos_sim)

        print(f"{label}: å¹³å‡ç»å¯¹CosSim = {avg_abs_cos_sim:.4f}")

    # ä¿å­˜ç»“æœ
    os.makedirs("image", exist_ok=True)
    np.savez("image/avg_abs_cos_sim_vs_cs_similarity.npz", Cs=Cs_values, avg_abs_cos_sims=avg_abs_cos_sims)

    # ç”»å›¾
    import matplotlib.pyplot as plt
    plt.figure(figsize=(10, 8))
    plt.plot(Cs_values, avg_abs_cos_sims, 'b-o', linewidth=10, markersize=25)
    plt.xlabel(r'Shuffling Probability $C_m$', labelpad=15)
    plt.ylabel('Average Absolute Cosine Similarity', labelpad=15)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.savefig("image/avg_abs_cos_sim_vs_cs_similarity.png", dpi=300, bbox_inches='tight')
    plt.close()

    print("âœ… è®¡ç®—å¹¶ä¿å­˜å®Œæˆï¼")
    print("ğŸ“ ä¿å­˜çš„æ–‡ä»¶:")
    print("  - image/avg_abs_cos_sim_vs_cs_similarity.npz")
    print("  - image/avg_abs_cos_sim_vs_cs_similarity.png")

if __name__ == "__main__":
    main()

# %%
import numpy as np
import pandas as pd

def load_and_compare_npz():
    # è¯»å–ä¸¤ä¸ªnpzæ–‡ä»¶
    random_data = np.load("image/avg_abs_cos_sim_vs_cs.npz")
    similarity_data = np.load("image/avg_abs_cos_sim_vs_cs_similarity.npz")
    
    # æå–æ•°æ®
    random_cs = random_data['Cs']
    random_cos_sim = random_data['avg_abs_cos_sims']
    
    similarity_cs = similarity_data['Cs']
    similarity_cos_sim = similarity_data['avg_abs_cos_sims']
    
    # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
    comparison_df = pd.DataFrame({
        'Cs_Value': random_cs,
        'Random_Shuffle': random_cos_sim,
        'Similarity_Shuffle': similarity_cos_sim,
        'Difference': similarity_cos_sim - random_cos_sim,
        'Relative_Change(%)': ((similarity_cos_sim - random_cos_sim) / random_cos_sim) * 100
    })
    
    # æ ¼å¼åŒ–æ˜¾ç¤º
    print("=" * 80)
    print("ğŸ“Š Random Shuffle vs Similarity-based Shuffle å¯¹æ¯”è¡¨")
    print("=" * 80)
    print(f"{'Cså€¼':<8} {'éšæœºäº¤æ¢':<12} {'ç›¸ä¼¼åº¦äº¤æ¢':<12} {'å·®å€¼':<12} {'ç›¸å¯¹å˜åŒ–(%)':<12}")
    print("-" * 80)
    
    for _, row in comparison_df.iterrows():
        print(f"{row['Cs_Value']:<8.1f} {row['Random_Shuffle']:<12.4f} {row['Similarity_Shuffle']:<12.4f} "
              f"{row['Difference']:<12.4f} {row['Relative_Change(%)']:<12.2f}")
    
    print("=" * 80)
    
    # ä¿å­˜ä¸ºCSVæ–‡ä»¶
    comparison_df.to_csv("image/shuffle_comparison_table.csv", index=False, float_format='%.4f')
    print("ğŸ“ è¡¨æ ¼å·²ä¿å­˜ä¸º: image/shuffle_comparison_table.csv")
    
    return comparison_df

if __name__ == "__main__":
    df = load_and_compare_npz()

# %%



